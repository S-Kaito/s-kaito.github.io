# Attention Is All You Need
https://arxiv.org/pdf/1706.03762.pdf

## 概要
現在とても覇権を握るモデルといえばエンコーダ・デコーダモデルを含む多くの畳み込みか再帰的ニューラルネットワークである。
パフォーマンスを誇るモデルはエンコーダデコーダモデルの中に何らかのアテンションの仕組みがある。
そこで、私たちは新たにシンプルな構築のネットワークである**Transformer**を提案する。
これは単独でアテンションメカニズムを持ち、再帰と畳み込みのメカニズムの調整を行える。
２台のPCを用いた翻訳作業の実験から、これらは優れたモデルであり並列可能なため学習の時間の減少も可能である。
私たちのモデルは**28.4 BLEU on the WMT 2014 English-to-Germanの翻訳(?)** で2BLEUの並列作業で今までより高い結果を出した。
*WMT2014英仏翻訳タスクにおいて、我々のモデルは、8つのGPUで3.5日間のトレーニングを行った後、41.8という新しいシングルモデルの最先端BLEUスコアを確立しました。*
これから膨大で限られたトレーニングデータを用いてTransformerが他タスクでも上手く機能する事を示したいと思う。

## 始めに
RNN, LSTM, gated recurrent NN等のSOTAと認定されたNNらは時系列モデルへのアプローチであり、言語モデルや機械翻訳問題の解決では*確固たる地位を築いている*。
多数のNNは再帰的言語モデルとエンコーダデコーダモデルの間で頑張ってきた。
