# Attention Is All You Need
https://arxiv.org/pdf/1706.03762.pdf

## 概要
現在とても覇権を握るモデルといえばエンコーダ・デコーダモデルを含む多くの畳み込みか再帰的ニューラルネットワークである。
一番のパフォーマンスを誇るモデルはエンコーダデコーダモデルの中にアテンションの仕組みがある。
私たちは新たにシンプルな構築のネットワークである**Transformer**を提案する。
これは単独でアテンションメカニズムを持ち、再帰と畳み込みのメカニズムの調整を行える。
２台のPCを用いた翻訳作業の実験からこれらは優れたモデルであり並列可能なため学習の時間の減少も可能である。
私たちのモデルは**28.4 BLEU on the WMT 2014 English-to-Germanの翻訳(?)**で2BLEUの並列作業で今までより高い結果を出した。
