# Attention Is All You Need
https://arxiv.org/pdf/1706.03762.pdf

## 概要
現在覇権を握るモデルといえばエンコーダ・デコーダモデルを含む多くの畳み込みか再帰的ニューラルネットワークである。
パフォーマンスを誇るモデルはエンコーダデコーダモデルの中に何らかのアテンションの仕組みがある。
そこで、私たちは新たにシンプルな構築のネットワークである**Transformer**を提案する。
これは単独でアテンションメカニズムを持ち、再帰と畳み込みのメカニズムの調整を行える。
２台のPCを用いた翻訳作業の実験から、これらは優れたモデルであり並列可能なため学習の時間の減少も可能である。
私たちのモデルは**28.4 BLEU on the WMT 2014 English-to-Germanの翻訳(?)** で2BLEUの並列作業で今までより高い結果を出した。
*WMT2014英仏翻訳タスクにおいて、我々のモデルは、8つのGPUで3.5日間のトレーニングを行った後、**41.8という新しいシングルモデルの最先端BLEUスコアを確立**しました。*
これから膨大で限られたトレーニングデータを用いてTransformerが他タスクでも上手く機能する事を示したいと思う。

## 始めに
RNN, LSTM, gated recurrent NN等のSOTAと認定されたNNらは時系列モデルへのアプローチであり、言語モデルや機械翻訳問題の解決では*確固たる地位を築いている。*
多数のNNは再帰的言語モデルとエンコーダデコーダモデルの間で頑張ってきた。

*リカレントモデルは通常、入力系列と出力系列のシンボル位置に沿って計算を因数分解します。* 
コンピュータで計算するときの位置合わせとして状態`h_t` 、前回の状態`h_t-1`があり、ポジション`t`が与えられる。訓練時にはこれを並列化のために取り除いてから行う。なぜなら、時系列データの長さに従いメモリも喰われるためである。
特徴抽出の小技や条件付きの計算などにより計算の効率はどんどん改善されている。(このケースではパフォーマンスは後者のほうが高い)
*しかし、逐次計算の基本的な制約は残っています。*

*注意メカニズムは、様々なタスクにおける説得力のあるシーケンスモデリングや変換モデルの不可欠な部分となっており、入力シーケンスや出力シーケンスの距離に関係なく依存関係のモデリングを可能にしています。
しかし、少数のケースを除いて、そのような注意メカニズムはリカレントネットワークと組み合わせて使用されています。
*

今回、私たちは再帰をせず、アテンションのメカニズムを用いて入出力の依存関係を表現する**Transformer**モデルを提案する。
Transformerは並列計算が可能で翻訳分野でのSOTAに到達可能である。それも計算時間は8つのP100 GPUを使って12時間しか使わない。

## 背景
時系列計算をしないための手法としてExtended Neural GPU, ByteNet, ConvS2Sがあるが、これらはCNNを用いている。これらはすべての隠れ層で入力と出力位置の計算を並列に行います。
*これらのモデルでは、2つの任意の入力位置または出力位置からの信号を関連付けるのに必要な演算数は、位置間の距離で増加し、ConvS2Sでは線形に、ByteNetでは対数的に増加します。*
これは位置の依存関係の学習を困難にしています。
*トランスフォーマーでは、注意力で加重された位置を平均化することで有効分解能が低下しますが、これは3.2節で説明*しますが*、マルチヘッドアテンションで打ち消す*ことができます。

セルフ-アテンション(イントラアテンションとも呼ばれます)とはアテンションメカニズムであり、時系列データの表現の*計算の*ために単純系列のなかの異なる2つの位置の関係付けを行います。
セルフ-アテンションは様々な分野(理解・抽象化・要約・タスクに依存しない分表現の学習など)で使われました。

*エンドツーエンドの記憶ネットワークは、シーケンスに沿った再帰ではなく、再帰注意のメカニズムに基づいており、*シンプルな言語でのQ&Aでも良いパフォーマンスを出しています。

*しかし、我々の知る限りでは、Transformerは、シーケンスに沿ったRNNや畳み込みを使用せずに、入力と出力の表現を計算するために、自己注意に頼った最初の伝達モデルです。*
これからのセクションではTransformerについての説明や、セルフアテンションのモチベーション、他モデルと比べた際のアドバンテージについて話していきます。
