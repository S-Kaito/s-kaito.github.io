<!DOCTYPE html>
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Language" content="ja" />
    <meta http-equiv="Content-Style-Type" content="text/css" />
    <meta http-equiv="Content-Script-Type" content="text/javascript" />
	<meta name="viewport" content="width=device-width,initial-scale=1">
	<link rel="stylesheet" type="text/css" href="../style.css">
</head>
<body>
	<div class="wrapper">
		<div class="main">
			<h1 style="text-align: center">Hello , DeepLearning !!</h1>
			<br>
			<h2>
				ディープラーニングとは？
			</h2>
			<div>
				&nbsp;ディープラーニングとはAI(Artifical Intelligence)の一種です。 生物の脳を参考にしたアルゴリズムです. 脳は "ニューロン" というものの集合で形作られています。 ニューロンは電気信号を一定量受けると他のニューロンに電気信号を渡します。 これが脳の仕組みです。
			</div>
			<div>
				&nbsp;ディープラーニングはこの仕組みをプログラムで起こしたものです。
			</div>
			<h2>
				ニューロンとは？
			</h2>
			<div>
				&nbsp;ニューロン無しではディープラーニングを語れません。 しかし単純なので安心してください。 Figure01を見てください.
			</div>
			<div class="image">
				<img src="Figure01.png" style="border: #000000 solid 2px;">
				<div><b>Figure01</b></div>
			</div>
			<div>
				&nbsp;ニューロンは電気信号を "一定量" 受けた時、他のニューロンに同じように電気信号を流します。 この"一定量"はニューロンによって変わります。
			</div>
			<div class="image">
				<img src="Figure02.png" style="border: #000000 solid 2px;">
				<div><b>Figure02</b></div>
			</div>
			<div>
				&nbsp;Figure02 はニューロンの模式図です。 "w" は重みです。それぞれのニューロンの繋がりの強さを表しています。 人間もみんながみんな仲良しではないようにニューロンにも関係性の強さが存在します。 "x" は他のニューロンから受けた電気信号で "w" と乗算します。
			</div>
			<div>
				&nbsp;x * w に "b" を足し合わせます。 これでニューロンの電気信号の完成します。 Figure03 がある一つのニューロンと別のニューロンの電気信号の公式で、 Figure03.5 が一つのニューロンが受ける電気信号の総和です。
			</div>
			<div class="image">
				<img src="Figure03.png" style="border: #000000 solid 2px;">
				<div><b>Figure03</b></div>
			</div>
			<div class="image">
				<img src="Figure03.5.png" style="border: #000000 solid 2px;">
				<div><b>Figure03.5</b></div>
			</div>
			<div>
				&nbsp;ニューロンの興奮(電気信号を渡すかどうか)の仕方はニューロンそれぞれです。 受け取った電気信号の合計でニューロンは興奮するかどうかを決定します。 これから、興奮の仕方に関するグラフをいくつか紹介します(簡単なグラフなので数学アレルギーの人もまだ吐き気はこらえて下さい。)
			</div>
			<div class="image">
				<img src="Figure04.png" style="border: #000000 solid 2px;">
				<div><b>Figure04</b></div>
			</div>
				&nbsp;Figure04 は "シグモイド関数" と呼ばれているものです。 この関数では0~1の中で値が落ち着きます。 なのでとても扱いやすいです。 何故扱いやすいかは後で記述するのではないでしょうか？
			<div class="image">
				<img src="Figure05.png" style="border: #000000 solid 2px;">
				<div><b>Figure05</b></div>
			</div>
				&nbsp; Figure05 は "ReLU" という関数です。まあこれもシンプルですね。 値が０以下にならないので変な学習の仕方をしません。 なのでよくディープラーニングで使われます。

			<div class="image">
				<img src="Figure06.png" style="border: #000000 solid 2px;">
				<div><b>Figure06</b></div>
			</div>
			<div>
				&nbsp;これについては説明は不要ですね。 <span style="color:#FCFCFC;">冗談です。 これもとても重要です。 これは"恒等関数"というものです。 これとソフトマックス関数は出力層でよく使われます。 特にこれは回帰問題(入力Xに対する出力Yを求めるような問題)によく使われます。理由は上限が無いからです。 </span> ('O')hohohoho.
			</div>
			<h2>
				ニューラルネットワーク
			</h2>
			<div>
				&nbsp;ニューロンをたくさん繋げた時のことを想像してください。 ニューロンは情報を電気信号で伝えます。 それがたくさん集まるので高度な計算が出来るのです。 ニューロンをたくさん繋げたものをニューラルネットワークと呼びます。 "ディープラーニング"とは特に入力層と出力層の間に沢山層があるようなニューラルネットワークをそう呼びます。 (つまり1層でもディープラーニングとは呼べそうですね。ただただ「深い」ってだけなので)
			</div>
			<div class="image">
				<img src="Figure07.png" style="border: #000000 solid 2px;">
				<div><b>Figure07</b></div>
			</div>
				&nbsp;"順伝播(propagation)"は予測に使い、"逆伝播(back propagation)" は学習するときに使います。
			<h2>
				実装しよう！ (propagation edition)
			</h2>
			<div>
				I'll implement for demonstration of propagation. This time I think I use simple network. Show Figure08. Maybe, you might think that is too simple. But even it can express things such as sine function , cos function and so on. Seeing is believing. Let's implement. Sample01 is source code.
			</div>
			<div>
				<b>Conditions</b>
				<li>Middle Layer = Sigmoid function</li>
				<li>Output Layer = identity function</li>
			</div>
			<div class="image">
				<img src="Figure08.png" style="border: #000000 solid 2px;">
				<div><b>Figure08</b></div>
			</div>
			<details class="code">
				<summary>Sample01(python)</summary>
				<a href="Sample01.py" download="Sample01.py" class="download">Download</a>
					<pre>
						<code>
import numpy as np
import matplotlib.pyplot as plt

INPUT_DATA = np.linspace(-1.0,1.0,100)
WEIGHT = 0.1

class Layer:
    def __init__(this,input_num,output_num):
        this.w = WEIGHT * np.random.randn(input_num,output_num)
        this.b = WEIGHT * np.random.randn(output_num)
        
    def forward(this,x):
        this.y = np.dot(x,this.w) + this.b

class MiddleLayer(Layer):
    def forward(this,x):
        super().forward(x)
        this.y = 1/(1+(np.exp(-this.y)))

layer = MiddleLayer(1,3)
output = Layer(3,1)

layer.w = np.array([[ 5,2,-5]])
layer.b = np.array([ 0.1,0.2,-1])

output.w = np.array([9,3,4])
output.b = np.array([-5])

plot_x = []
plot_y = []

for i in INPUT_DATA:
    layer.forward(np.array([i]).reshape(1,1))
    output.forward(layer.y)

    plot_x.append(i)
    plot_y.append(output.y)

plt.scatter(plot_x, plot_y, marker="+")
plt.show()
</code></pre>
			</details>
			<div>
				The first, I'll explain Layer class and MiddleLayer class which important part of this source code. All layers perform the same operation. So, MiddleLayer class and OutputLayer is extended Layer class. (If you can't understand that I said, I want you to study about extension.When I speak real intention, I do not want to explain it.) __init__ method is the constructor which define first neuron's condition. this.w is weight (Look at Figure02 or Figure03). this.b is bias. They have not learned yet. They are learning when neural network does back propagation. Next, foward method is to do propagation. When Layer class calls foward function, it's doing propagation with identity function. But MiddleLayer doing with figmoid function. It's as stipulated by the conditions. 
			</div>
			<div class="code">
				<div class="source-title"><b>Sample02(Python)</b></div>
				<pre class="source"><code>
class Layer:
    def __init__(this,input_num,output_num):
        this.w = WEIGHT * np.random.randn(input_num,output_num)
        this.b = WEIGHT * np.random.randn(output_num)
        
    def forward(this,x):
        this.y = np.dot(x,this.w) + this.b

class MiddleLayer(Layer):
    def forward(this,x):
        super().forward(x)
        this.y = 1/(1+(np.exp(-this.y)))

</code></pre>
			</div>
			<div>
				The next, I'll explain calcation of neuron. It's easy to use matrix calcation. Propagation can be impermented with this source code. It calcates Figure03.
			</div>
			<div>
				<div class="source-title"><b>Sample03(Python)</b></div>
				<pre class="source"><code>
this.y = np.dot(x,this.w) + this.b

</code></pre>
			</div>
			<div>
				This is the program which is initializing neurons. This parameters dicides. Try to change parameters to Sample05. You can show sine function.
			</div>
			<div class="code">
				<div class="source-title"><b>Sample04(Python)</b></div>
				<pre class="source"><code>
layer.w = np.array([[ 5,2,-5]])
layer.b = np.array([ 0.1,0.2,-1])

output.w = np.array([9,3,4])
output.b = np.array([-5])

</code></pre>
			</div>
			<div class="code">
				<div class="source-title"><b>Sample05(Python)</b></div>
				<pre class="source"><code>
layer.w = np.array([[ 1.5226207,-4.74313941,-5.09765436]])
layer.b = np.array([ 0.05025657,0.25530377,-1.12453411])

output.w = np.array([9.01367277,5.04463465,1.2634446 ])
output.b = np.array([-7.73053153])

</code></pre>
			</div>
			<div>
				This is main function. It instructs to do forwarding and backwarding element count of INPUT_DATA times. And, it saves output data to plot_x and plot_y. The final, it shows output data with graph like Figure09.
			</div>
			<div class="code">
				<div class="source-title"><b>Sample06(Python)</b></div>
				<pre class="source"><code>
for i in INPUT_DATA:
    layer.forward(np.array([i]).reshape(1,1))
    output.forward(layer.y)

    plot_x.append(i)
    plot_y.append(output.y)


plt.scatter(plot_x, plot_y, marker="+")
plt.show()

</code></pre>
			</div>
			<div class="image">
				<img src="Figure09.png" style="border: #000000 solid 2px;">
				<div><b>Figure09</b></div>
			</div>
			<h2>
				What is Back propagation?
			</h2>
			<div>
				The next, I'll explain back propagation. Back propagation is propagating error to input layer from output layer. For example, the neuron forwards 1. But another neuron wants 2. Then the neuron ramps up relationship like 1.1 , 1.01 and so on. Or ramping up bias. The feature of it is Figure10. 
			</div>
			<details class="code">
				<summary>Sample07(python)</summary>
				<a href="Sample07.py" download="Sample07.py" class="download">Download</a>
					<pre>
						<code>
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

INPUT_DATA = np.linspace(-1.0,1.0,100)
OUTPUT_DATA = np.sin(INPUT_DATA * np.pi)
WEIGHT = 0.1
EPOCH = 2000

class Layer:
    def __init__(this,input_num,output_num):
        this.w = WEIGHT * np.random.randn(input_num,output_num)
        this.b = WEIGHT * np.random.randn(output_num)
        
    def forward(this,x):
        this.x = x
        this.y = np.dot(x,this.w) + this.b

    def backward(this,x):
        delta = this.y - x

        this.grad_w = np.dot(this.x.T,delta)
        this.grad_b = np.sum(delta,axis=0)

        this.grad_x = np.dot(delta,this.w.T)

    def update(this):
        this.w -= this.grad_w * WEIGHT
        this.b -= this.grad_b * WEIGHT

class MiddleLayer(Layer):
    def forward(this,x):
        super().forward(x)
        this.y = 1/(1+(np.exp(-this.y)))

    def backward(this,x):  
        delta = x * (1 - this.y) * this.y

        this.grad_w = np.dot(this.x.T,delta)
        this.grad_b = np.sum(delta,axis=0)

        this.grad_x = np.dot(delta,this.w.T)

layer = MiddleLayer(1,3)
output = Layer(3,1)

fig = plt.figure()

img = []
    
for j in range(EPOCH):
    plot_x = []
    plot_y = []
    for i in INPUT_DATA:
        layer.forward(np.array([i]).reshape(1,1))
        output.forward(layer.y)

        plot_x.append(i)
        plot_y.append(np.sum(output.y))

        output.backward(OUTPUT_DATA[np.where(INPUT_DATA == i)].reshape(1,1))
        layer.backward(output.grad_x)

        layer.update()
        output.update()

    if j % (EPOCH / 20) == 0:
        print(j + (EPOCH / 20)," / ",EPOCH)
        print(layer.w,layer.b)
        print(output.w,output.b)
        plt.plot(INPUT_DATA,OUTPUT_DATA,linestyle="dashed")
        line = plt.scatter(plot_x,plot_y,marker="+")
        img.append(line)
plt.show()
</code></pre></details>
			<div>
				Most important code is method of backpropagation. The methods forwards error to other neuron. Other neuron approaches parameter "w" and "b". Repeating many times, network can learn sine function. Sample08 is code of backpropagating. The variable delta is defference between output and answer. The variables grad_w and grad_b are also defference. The update method approaches weight and bias from output to answer. By doing this, the network will be smart.
			</div>
<div class="code">
				<div class="source-title"><b>Sample08(Python)</b></div>
				<pre class="source"><code>
def backward(this,x):
    delta = this.y - x

    this.grad_w = np.dot(this.x.T,delta)
    this.grad_b = np.sum(delta,axis=0)

    this.grad_x = np.dot(delta,this.w.T)

def update(this):
    this.w -= this.grad_w * WEIGHT
    this.b -= this.grad_b * WEIGHT
</code></pre>
			</div>
			<div>
				Let's run Sample 09! You can see a network that will be smarter and smarter.
			</div>
			<details class="code">
				<summary>Sample09(python)</summary>
				<a href="Sample09.py" download="Sample09.py" class="download">Download</a>
					<pre>
						<code>

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import tkinter

INPUT_DATA = np.linspace(-1.0,1.0,100)
OUTPUT_DATA = np.sin(INPUT_DATA * np.pi)
WEIGHT = 0.1
EPOCH = 2000

class Layer:
    def __init__(this,input_num,output_num):
        this.w = WEIGHT * np.random.randn(input_num,output_num)
        this.b = WEIGHT * np.random.randn(output_num)
        
    def forward(this,x):
        this.x = x
        this.y = np.dot(x,this.w) + this.b

    def backward(this,x):
        delta = this.y - x

        this.grad_w = np.dot(this.x.T,delta)
        this.grad_b = np.sum(delta,axis=0)

        this.grad_x = np.dot(delta,this.w.T)

    def update(this):
        this.w -= this.grad_w * WEIGHT
        this.b -= this.grad_b * WEIGHT

class MiddleLayer(Layer):
    def forward(this,x):
        super().forward(x)
        this.y = 1/(1+(np.exp(-this.y)))

    def backward(this,x):  
        delta = x * (1 - this.y) * this.y

        this.grad_w = np.dot(this.x.T,delta)
        this.grad_b = np.sum(delta,axis=0)

        this.grad_x = np.dot(delta,this.w.T)

layer = MiddleLayer(1,3)
output = Layer(3,1)

def event():
    global EPOCH

    EPOCH -= 1
    for i in INPUT_DATA:
        layer.forward(np.array([i]).reshape(1,1))
        output.forward(layer.y)

        output.backward(OUTPUT_DATA[np.where(INPUT_DATA == i)].reshape(1,1))
        layer.backward(output.grad_x)

        layer.update()
        output.update()

    if EPOCH % 20 == 0:    
        print(layer.w,layer.b)
        print(output.w,output.b)

    layer_b = layer.b.tolist()
    layer_w = layer.w.tolist()
    output_w = output.w.tolist()
    output_b = output.b.tolist()

    canvas.create_line(350,100,150,350,fill=getColor(layer_w[0][0]),width=3.0)
    canvas.create_line(350,100,350,350,fill=getColor(layer_w[0][1]),width=3.0)
    canvas.create_line(350,100,550,350,fill=getColor(layer_w[0][2]),width=3.0)

    canvas.create_line(150,350,350,550,fill=getColor(output_w[0][0]),width=3.0)
    canvas.create_line(350,350,350,550,fill=getColor(output_w[1][0]),width=3.0)
    canvas.create_line(550,350,350,550,fill=getColor(output_w[2][0]),width=3.0)

    canvas.create_oval(100,300,200,400,fill=getColor(layer_b[0]))
    canvas.create_oval(300,300,400,400,fill=getColor(layer_b[1]))
    canvas.create_oval(500,300,600,400,fill=getColor(layer_b[2]))

    canvas.create_oval(300,500,400,600,fill=getColor(output_b[0]))

    if EPOCH > 0:
        root.after(10,event)

def getColor(n):
    if n < -1:
        return "#FF0000"
    elif n < -0.25:
        return "#FF8888"
    elif n < 0.25:
        return "#FFFFFF"
    elif n < 1:
        return "#88FF88"
    else:
        return "#00FF00"

root = tkinter.Tk()
root.geometry("700x700")
canvas = tkinter.Canvas(root,width=700,height=700)
canvas.place(x=0,y=0)
root.after(10,event)
root.mainloop()
</code></pre></details>
			<h2>
				Let's imprement for Keras!
			</h2>
			<div>
				Keras is libraly for Neural Network. It is easier than not using to imprement Neural Network. Immediately, show you sample code. It learn sine function. It is the same as Sample 07.
			</div>
			<details class="code">
				<summary>Sample09(python)</summary>
				<a href="Sample09.py" download="Sample09.py" class="download">Download</a>
					<pre>
						<code>
import keras
from keras import optimizers
from keras import losses
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.utils import plot_model
import matplotlib.pyplot as plt
import numpy as np

INPUT_DATA = np.linspace(-1.0,1.0,100)
OUTPUT_DATA = np.sin(INPUT_DATA * np.pi)
EPOCH = 10

model = Sequential()
model.compile(loss='mean_absolute_error', optimizer="adam")
model.add(Dense(3,activation="sigmoid",input_shape=(1,)))
model.add(Dense(1,activation="linear"))
model.summary()

for j in range(1,EPOCH):
	plot_x = []
	plot_y = []

	for i in INPUT_DATA:
		output = model.predict(np.array([i]))
		plot_x.append(i)
		plot_y.append(output)

	model.fit(INPUT_DATA,OUTPUT_DATA,batch_size=len(INPUT_DATA),verbose=0,epochs=EPOCH * 300)

	plt.plot(INPUT_DATA,OUTPUT_DATA,linestyle="dashed")
	plt.scatter(plot_x,plot_y,marker="+")
	plt.show()

for i in INPUT_DATA:
	output = model.predict(np.array([i]))
	plot_x.append(i)
	plot_y.append(output)
</code></pre></details>
			<div>
				What a wonderful! That long source code became to too shorter! Let's see the Keras, and you will feel useful.
			</div>
			<div class="source-title"><b>Sample08(Python)</b></div>
				<pre class="source"><code>
model = Sequential()
model.compile(loss='mean_absolute_error', optimizer="adam")
model.add(Dense(3,activation="sigmoid",input_shape=(1,)))
model.add(Dense(1,activation="linear"))
model.summary()
</code></pre>
			<div>
				It is initialise the network.First, instance "model" is network. Second, the method "compile" is desiding loss function, optimizer, and so on. Third, the method "add" is added layer class that is "Dense". Finally, exectute "summary" method, network is decided.
			</div>
			<h2>
				Reference
			</h2>
			<div>
				<li>
					<b>はじめてのディープラーニング</b><br>
					　著　者：我妻　幸長<br>
					　発行所：SBクリエイティブ<br>
				</li>
			</div>
		</div>
	</div>
</body>
</html>
