<!DOCTYPE html>
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Language" content="ja" />
    <meta http-equiv="Content-Style-Type" content="text/css" />
    <meta http-equiv="Content-Script-Type" content="text/javascript" />
	<meta name="viewport" content="width=device-width,initial-scale=1">
	<link rel="stylesheet" type="text/css" href="../style.css">
</head>
<body>
	<div class="wrapper">
		<div class="main">
			<h1 style="text-align: center">Hello , DeepLearning !!</h1>
			<br>
			<h2>
				What is DeepLearning?
			</h2>
			<div>
				&nbsp;DeepLearning is one of AI(Artifical Intelligence). It is imitated brains of living beings. Brains of living beings is made up a lot of a group of neuron. When neuron receive eloctronic signals, the neuron give electronic signals to another neuron. This mechanism is used in the brain.
			</div>
			<div>
				&nbsp;DeepLearning can be implemented to reproduce neuron's mecanism.
			</div>
			<h2>
				What is neuron?
			</h2>
			<div>
				&nbsp;We can't talk about DeepLearning without neuron. But neuron's nechanism is simple. Look at Figure01.
			</div>
			<div class="image">
				<img src="Figure01.png" style="border: #000000 solid 2px;">
				<div><b>Figure01</b></div>
			</div>
			<div>
				&nbsp;If neuron receive electronic signals to another neuron above the standard, the neuron will be excitement and give electronic signals another neuron. But each neuron has difficult standard. 
			</div>
			<div class="image">
				<img src="Figure02.png" style="border: #000000 solid 2px;">
				<div><b>Figure02</b></div>
			</div>
			<div>
				&nbsp;Figure02 is the formura which neuron received electronic signals. "w" is weight (in other words is value). It means strength of neuron-to-neuron relationship. Neurons are not always friends as neurons are not as good friends as all people are. So, "x" which receive electronic signal other neuron is multipled "w".
			</div>
			<div>
				&nbsp;It is added "b" too. Neuron's excitement are disjoined. So the formula has to conform it. Figure03 is formula all of electronic signals it received.And Figure03.5 is schematic of a neuron.
			</div>
			<div class="image">
				<img src="Figure03.png" style="border: #000000 solid 2px;">
				<div><b>Figure03</b></div>
			</div>
			<div class="image">
				<img src="Figure03.5.png" style="border: #000000 solid 2px;">
				<div><b>Figure03.5</b></div>
			</div>
			<div>
				&nbsp;Neuron's feature is descripted the above. The next work is express neuron's excitement from total of electronic signals. I'll show you graphs which types of excitementof neuron.
			</div>
			<div class="image">
				<img src="Figure04.png" style="border: #000000 solid 2px;">
				<div><b>Figure04</b></div>
			</div>
				&nbsp;Look at Figure04. This function's name is "Sigmoid function". There is feature which solution can fit 0 to 1. In addtion, differential of this function is easy to handle. For that reason, it is used on DeepLearning since ancient times.
			<div class="image">
				<img src="Figure05.png" style="border: #000000 solid 2px;">
				<div><b>Figure05</b></div>
			</div>
				&nbsp;Look at Figure05. This function's name is "ReLU". It's simple, too. And, It is possible to learn more stable when the network of neuron is complexity. There is also merit which f'(x) &isin; +<b>Z</b> is consist. So that, it's used by DeepLearning. 
			<div class="image">
				<img src="Figure06.png" style="border: #000000 solid 2px;">
				<div><b>Figure06</b></div>
			</div>
			<div>
				&nbsp;Should I say something? <span style="color:#FCFCFC;">Sorry, it's a joke. This function is also important. It's called Identity function. It or softmax function are commonly used in the output layer of neural network. Especially, it's suitable to solve regression problem because there is no limit on the output range and it is continuous. </span>
			</div>
			<h2>
				Network of neuron
			</h2>
			<div>
				&nbsp;Imagine if you connect many neuron. Neurons can send many information as electronic signals. And can calcuate advance calcuation as possible as image proccesing. Network of neuron is called neural network. DeepLearning is thing became neural network deeper. Therefore all neural network can called DeepLearning even if it is single layer. 
			</div>
			<div class="image">
				<img src="Figure07.png" style="border: #000000 solid 2px;">
				<div><b>Figure07</b></div>
			</div>
				&nbsp;Propagation is trying achievement of learning. Against, backpropagation is learning from difference of error with answer. 
			<h2>
				Let's implement (propagation edition)
			</h2>
			<div>
				I'll implement for demonstration of propagation. This time I think I use simple network. Show Figure08. Maybe, you might think that is too simple. But even it can express things such as sine function , cos function and so on. Seeing is believing. Let's implement. Sample01 is source code.
			</div>
			<div>
				<b>Conditions</b>
				<li>Middle Layer = Sigmoid function</li>
				<li>Output Layer = identity function</li>
			</div>
			<div class="image">
				<img src="Figure08.png" style="border: #000000 solid 2px;">
				<div><b>Figure08</b></div>
			</div>
			<details>
				<summary>Sample01(python)</summary>
				<pre><code>
import numpy as np
import matplotlib.pyplot as plt

INPUT_DATA = np.linspace(-1.0,1.0,100)
WEIGHT = 0.1

class Layer:
    def __init__(this,input_num,output_num):
        this.w = WEIGHT * np.random.randn(input_num,output_num)
        this.b = WEIGHT * np.random.randn(output_num)
        
    def forward(this,x):
        this.y = np.dot(x,this.w) + this.b

class MiddleLayer(Layer):
    def forward(this,x):
        super().forward(x)
        this.y = 1/(1+(np.exp(-this.y)))

layer = MiddleLayer(1,3)
output = Layer(3,1)

layer.w = np.array([[ 5,2,-5]])
layer.b = np.array([ 0.1,0.2,-1])

output.w = np.array([9,3,4])
output.b = np.array([-5])

plot_x = []
plot_y = []

for i in INPUT_DATA:
    layer.forward(np.array([i]).reshape(1,1))
    output.forward(layer.y)

    plot_x.append(i)
    plot_y.append(output.y)

print(layer.w)

plt.scatter(plot_x, plot_y, marker="+")
plt.show()
</code></pre>
			</details>
			<div>
				The first, I'll explain Layer class and MiddleLayer class which important part of this source code. All layers perform the same operation. So, MiddleLayer class and OutputLayer is extended Layer class. (If you can't understand that I said, I want you to study about extension.When I speak real intention, I do not want to explain it.) __init__ method is the constructor which define first neuron's condition. this.w is weight (Look at Figure02 or Figure03). this.b is bias. They have not learned yet. They are learning when neural network does back propagation. Next, foward method is to do propagation. When Layer class calls foward function, it's doing propagation with identity function. But MiddleLayer doing with figmoid function. It's as stipulated by the conditions. 
			</div>
			<div>
				<div class="source-title"><b>Sample02(Python)</b></div>
				<pre class="source"><code>
class Layer:
    def __init__(this,input_num,output_num):
        this.w = WEIGHT * np.random.randn(input_num,output_num)
        this.b = WEIGHT * np.random.randn(output_num)
        
    def forward(this,x):
        this.y = np.dot(x,this.w) + this.b

class MiddleLayer(Layer):
    def forward(this,x):
        super().forward(x)
        this.y = 1/(1+(np.exp(-this.y)))

</code></pre>
			</div>
			<div>
				The next, I'll explain calcation of neuron. It's easy to use matrix calcation. Propagation can be impermented with this source code. It calcates Figure03.
			</div>
			<div>
				<div class="source-title"><b>Sample03(Python)</b></div>
				<pre class="source"><code>
this.y = np.dot(x,this.w) + this.b

</code></pre>
			</div>
			<div>
				This is the program which is initializing neurons. This parameters dicides. Try to change parameters to Sample05. You can show sine function.
			</div>
			<div class="code">
				<div class="source-title"><b>Sample04(Python)</b></div>
				<pre class="source"><code>
layer.w = np.array([[ 5,2,-5]])
layer.b = np.array([ 0.1,0.2,-1])

output.w = np.array([9,3,4])
output.b = np.array([-5])

</code></pre>
			</div>
			<div class="code">
				<div class="source-title"><b>Sample05(Python)</b></div>
				<pre class="source"><code>
layer.w = np.array([[ 1.5226207,-4.74313941,-5.09765436]])
layer.b = np.array([ 0.05025657,0.25530377,-1.12453411])

output.w = np.array([9.01367277,5.04463465,1.2634446 ])
output.b = np.array([-7.73053153])

</code></pre>
			</div>
			<div class="code">
				<div class="source-title"><b>Sample06(Python)</b></div>
				<pre class="source"><code>
for i in INPUT_DATA:
    layer.forward(np.array([i]).reshape(1,1))
    output.forward(layer.y)

    plot_x.append(i)
    plot_y.append(output.y)

</code></pre>
			</div>
		</div>
	</div>
</body>
</html>
